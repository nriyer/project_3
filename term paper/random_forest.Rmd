---
title: "Ensemble Methods"
author: "Nisha Iyer"
date: "April 1, 2016"
output: word_document
---

### I will use the well known Boston Housing Data set to build out a random forest model, followed
### by an AdaBoost model. I want to use the same data set for comparison of these two methods. 

```{r}
library(MASS)
library(caret)
library(randomForest)
library(ROCR)
library(miscTools)

#load Boston Housing data 
#This example will be regression trees since the target variable is continuous (median value).
data(Boston)
head(Boston)

dim(Boston)

#Use createDataPartition from the caret package to create train and test sets
set.seed(123)
split <- createDataPartition(y=Boston$medv, p = 0.7, list=FALSE)
train <- Boston[split,]
test<- Boston[-split,]

#A note about 'createDataPartition; createResample can be used to make a simple bootstrap and createFolds for 
#cross-validation groupings.
```
### OK, training and test sets have been created. Now we will run randomForest on the training data.

### The randomForest algorithm below includes target variable (medv) '.' which is all the predictors,
### the data is BostonTrain, number of trees to create using 'bagging' method is 100 and what makes the 
### random forest algorithm set apart from boosting; number of predictors randomly chosen from full set of
### predictors is 5. In regression trees, the recommended number for mtry is the total number of predictors
### divided by three. In classification (an example to follow) the recommended number is the square root of
### predictors. 

```{r}
#Build the model:
rf <- randomForest(medv~., data=train, mtry=6, importance = TRUE)
rf

#Now make predictions on the test set:
yhat <- predict(rf, test)

#The MSE:
mean((yhat - test$medv)^2)

#Look at variable importance; looking at increase in MSE by looking at
#mean decrease in accuracy in predictions on out
#of bag samples, when the given variable is excluded from the model and
#increase in node purity by looking at the total decrease in node purity 
#resulting from the given variable, averaged over all trees.
importance(rf)

#Now plot it:
varImpPlot(rf)


```

---
title: "adaboost"
author: "Nisha Iyer"
date: "April 2, 2016"
output: word_document
---
We will now use the iris data set to demonstrate the AdaBoost ensemble algorithm.

```{r}
library(adabag)
library(randomForest)
data(iris)
##adaBoost
data(iris)
head(iris)
set.seed(123)
split <- createDataPartition(y=iris$Species, p = 0.7, list=FALSE)
train <- iris[split,]
test<- iris[-split,]
train$Species <- factor(train$Species)
adaboost<-boosting(Species ~ . , data=train, boos=TRUE, mfinal=20, coeflearn='Breiman')
summary(adaboost)
# Above we use the Adaboost algorithm. M-final is the number of times the boosting algorithm is run. Breiman
# as the 'coeflearn' suggests that we are using the M1 algorithm proposed by Breiman. The M1 algorithm
# is a classification algorithm where each class can attain a weight of no more than 1/2.
adaboost$importance
#This gives importance of the variables.
errorevol(adaboost,test)
#This gives the error of the variables
predictions <- predict(adaboost,test)
predictions$confusion
predictions$error
#Let's compare AdaBoost to RandomForest. To do this, I will quickly run randomforest on the iris dataset.
rf_iris <- randomForest(Species ~ ., data = train)
rf_iris
yhat_iris <- predict(rf_iris, test)
#Random forest Confusion Matrix:
table(yhat_iris, test$Species)
#AdaBoost Confusion Matrix:
predictions$confusion
```


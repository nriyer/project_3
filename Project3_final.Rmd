---
title: 'Project #'
author: "Nisha Iyer, Rachel Jordan, Sam Dooley"
date: "April 29, 2016"
output: pdf_document
---

### For the complete set of documents, try the functions in lecture 7:

#####You can start by following the slides in Lecture 7.You should do at least the following:For the complete set of documents, try the functions in lecture 7. What happens? Does it yield anything understandable about the documents. [answered below]

```{r}
data("acq")
head(acq)

#compilation of 50 news articles with additional meta information form the 
#Reuters-21578 data se of topic acq. 13 documents
ACQ <- acq
ACQ

#this tell us what information (metadata) about our documents.  For example, how many chars are within each doc. 
inspect(ACQ) #all 50 docs
inspect(ACQ[1:2]) #just the first 2

meta(ACQ[[2]], "id") #this is another way to reference

#extract one document
text1 <- ACQ[[1]]
text1

#This function tells us more information about the texts (all 50)
#For example, the maximal term length, non/sparse entries 
ACQdoc <-DocumentTermMatrix(ACQ)
ACQdoc
nrow(ACQdoc) #50 rows
ncol(ACQdoc) #2103 cols
inspect(ACQdoc[1:6,1:10])

#termFreq tells us more about an individual doc/text such as term freq within the doc
test1tf <- as.data.frame(termFreq(text1))
#rank words most to least
rank_of_words <- cbind(as.data.frame(rownames(test1tf)),test1tf %>% arrange(desc(termFreq(text1))))

#the tm_map and content_transformer transforms the data 
#such as converting the terms to lower case
ACQlow <- tm_map(ACQ, content_transformer(tolower))
ACQlow

#the next function removes anything other than English letters or spaces 
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
ACQcl <- tm_map(ACQlow,content_transformer(removeNumPunct))
ACQcl

#after converting the text to lower case, and removing punctionation 
#we are going to remove stopwords (filler words such as a, an, the, etc.)
stopwords <- c(stopwords('english'))
ACQstop <- tm_map(ACQcl, removeWords, stopwords)

#here we can look at the first two text docs and see how the word count (char) differs
inspect(ACQ[1:2])#the original; 1 with 1287 chars, 2nd with 784 chars
inspect(ACQstop[1:2]) #the amount of words is much less; first with 1030 chars, second with 620 chars

#now we are putting the terms without punctuation and stopwords into a matrix
ACQdm2 <- DocumentTermMatrix(ACQstop, control= list(wordLenghts = c(1,Inf)))
ACQdm2


#find terms with a frequency of 5 or more
freq.terms <- findFreqTerms(ACQdm2, lowfreq=5)
freq.terms
#there are 201 terms with a frequency of 5 or more

#the Assocs function finds associations with terms, such as states or year
findAssocs(ACQdm2, "states", 0.25)
findAssocs(ACQdm2, "year", 0.25)

#Next we're going to put the terms with frequency count of 5 or more into a dataframe
term.freq <- rowSums(as.matrix(ACQdm2))
term.freq <- subset(term.freq, term.freq <= 5)
termdf <- data.frame(term = names(term.freq),freq=term.freq)
term_sort <- termdf %>% arrange(desc(freq))
term_sort[1:50,]

#plot top 50 frequent terms
ggplot(term_sort[1:50,], aes(x=term,y=freq)) + geom_bar(stat = "identity") + 
  xlab("Terms") + ylab("Count") + coord_flip()
```
##### What happens? Does it yield anything understandable about the documents 
###### Yes, the different functions allows us to break down the different text documents we were able to see how many stopwords and punctuation was included in the total character count of the texts the term frequencies allowed us insight into the top frequented words in the text the functions provided a lot of insight into the general documents, text, and words used in the texts

##### Find the 10 longest documents (in number of words).  
```{r}
#using quanteda for the next few questions
mycorpus <- corpus(acq)
summary_acq <- as.data.frame(summary(mycorpus))

#10 longest documents in the corpus
sort_top10 <- summary_acq %>% arrange(desc(Tokens))
top_10_docs <- subset(sort_top10, select=c(id, heading))[1:10,]
top10 <- top_10_docs[,1]

top10
topdocs <- mycorpus[mycorpus$documents$id %in% top10]
topdocs


#top 10 dendogram, 1 for each of the top 10 documents
top10.dendogram <- function(tdm2,doc)
{
  acq.mat <- as.matrix(t(tdm2))
  acq.mat <- as.data.frame(acq.mat)
  acq.mat <- acq.mat[,top10]
  acq.mat <- as.matrix(acq.mat)
  distMatrix <- dist(scale(acq.mat[,doc]))
  fit <- hclust(distMatrix, method = "ward.D2")
  print(plot(fit,main = "Dendogram"))
}
for (i in 1:10){
top10.dendogram(tdm2,i)
}

#word cloud for top 10
wordcloud.func <- function(ACQstop, doc)
{
  dtm <- TermDocumentMatrix(ACQstop)
  m <- as.data.frame(as.matrix(dtm))
  m <- m[,top10]
  m <- as.matrix(m)
  v <- sort(m[,doc],decreasing=TRUE)
  d <- data.frame(word = row.names(m),freq=v)
  set.seed(1234)
  print(wordcloud(words = d$word, freq = d$freq, min.freq = 1,
            max.words=200, random.order=FALSE, rot.per=0.35, 
            colors=brewer.pal(8, "Dark2")))
}
for (i in 1:10){
  wordcloud.func(ACQstop,i)
}
```

###### Prior to removing punctuation find the longest word and longest sentence in each of 10 docs my corpus is before removing punctuation

```{r}
#####FIND LONGEST WORD in 10 docs
max_length <- c()
word <- c()
id <- c()
for (i in 1:10){
  words <- tokenize_words(topdocs[[i]][[1]])
  word[i] <- words[nchar(words) == max(nchar(words))]
  max_length[i] <- max(nchar(words))
  id[i] <- names(topdocs[i])
}

final.longest_word <- data.frame(max_length = max_length,word=word,id = id)

#####FIND LONGEST SENTENCE in 10 docs
topdocs
topdocs[[2]]
names(topdocs[1])

#split into sentences
get_sentence_df_func <- function(x){
  sentence_df <- data.frame(sentence = character(0),
                            document = character(0))
  for (i in 1:10){
    temp <- data.frame(sentence=tokenize_sentences(x[[i]][[1]]),id=names(x[i]))
    sentence_df <- rbind(sentence_df,temp)
  }
  return(sentence_df)
}

#ALL sentences in the top 10 documents 
text_sent <- get_sentence_df_func(topdocs)

#word count for each sentence
text_sent$sentence <- as.character(text_sent$sentence)
count <- c()
sapply(strsplit(text_sent$sentence[23], " "), length)
for (i in 1:nrow(text_sent)){
  count[i] <- sapply(strsplit(text_sent$sentence[i], " "), length)
}

#length of each sentence in each document
count_sentences <- cbind(count,text_sent)

#top 10 lengths 
longest_10 <- count_sentences %>% group_by(id) %>% 
  arrange(desc(count)) %>% top_n(1,count) %>% distinct(id)

#remove punctuation for each sentence
#remove punctuation from topdocs
str(count_sentences$sentence)
count_sentences$sentence_nopunct[4] <- ( gsub("[[:punct:]]", "", count_sentences$sentence[4]) )
#loop to remove punctuation from each sentence
nopunct <- c()
for (i in 1:nrow(count_sentences)){
  nopunct[i] <-  ( gsub("[[:punct:]]", "", count_sentences$sentence[i]) )
}
#bind final output together together
final_nopunct_df <- cbind(nopunct,count_sentences)
```
##### The project helped us learn a lot about text analytics and key principals of analyzing unstructured text.We identified three key areas this project helped you learn about data science includes (1) the general approach to breaking down texts in R using Corpuses and tokens; (2) The exploratory analysis and derived insights that can be accomplish on a text documents through word counts, frequencies, associations, and character lengths; (3) we were able to learn how to apply data mining techniques to text analytics for deeper insights such as clustering (hierarchical and kmeans). 




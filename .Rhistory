plot(hc.complete,main="Complete Linkage", xlab="", sub="", cex=.9)
str(numeric)
cluster2 <- mush3
cluster2<-mush3[,-1]
row <- nrow(cluster2)
col <- col(cluster2)
row <- nrow(cluster)
col <- col(cluster)
col1 <- col(cluster2)
##train and test at 50-50 split
trainindex <- sample(row1, 0.7*row, replace=FALSE)
train_70 <- cluster2[trainindex,]
row1 <- nrow(cluster2)
col1 <- col(cluster2)
trainindex <- sample(row1, 0.7*row, replace=FALSE)
train_70 <- cluster2[trainindex,]
test_70 <- cluster2[-trainindex,]
dim(train_70)
train3 <- scale(train_70)
test3 <- scale(test_70)
train3 <- as.matrix(train3)
test3 <- as.matrix(test3)
wss_and_bss(train3)
kmeans_iyer(train3,3)
kmeans_iyer(test3,3)
kmeans_iyer(test2,7)
kmeans_iyer(train3,3)
kmeans_iyer(train3,5)
kmeans_iyer(train3,7)
kmeans_iyer(test3,7)
library(MASS)
library(dplyr)
library(rvgtest)
library(Hmisc)
library(reshape2)
library(caret)
library(FactoMineR)
setwd("~/Desktop/GW_spring2016/bigdata")
mr <- read.table('agaricus-lepiota.data',sep=",",header=FALSE)
library(MASS)
library(dplyr)
library(rvgtest)
library(Hmisc)
library(reshape2)
library(caret)
library(FactoMineR)
#set wd
setwd("~/Desktop/GW_spring2016/bigdata")
mr <- read.table('agaricus-lepiota.data',sep=",",header=FALSE)
str(mr)
scale <- function(df){
pre_range <- preProcess(df,method="range")
processed <- predict(pre_range,df)
return(data.frame(processed))
}
wss_and_bss <- function(df){
#within sum of squares
wss <- (nrow(df)-1)*sum(apply(df,2,var))
for (i in 1:12) wss[i] <- sum(kmeans(df,
centers=i)$withinss)
print(plot(1:12, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares"))
#between sum of squares
bss <- (nrow(df)-1)*sum(apply(df,2,var))
for (i in 1:12) bss[i] <- sum(kmeans(df,
centers=i)$betweenss)
print(plot(1:12, bss, type="b", xlab="Number of Clusters",
ylab="Between groups sum of squares"))
}
#define function for final output
kmeans_iyer <- function(df,k,n=20){
fit <-kmeans(df,k)
#get cluster means:this illustrates amount of each characteristic in each cluster
aggregate(df,by=list(fit$cluster), FUN=mean)
#append cluster assignment
cluster_assignment <- data.frame(df, fit$cluster)
#table of how many variables fit in each cluster
print(table(fit$cluster))
km.out<- kmeans(cluster_assignment,k,nstart=n)
print(km.out$tot.withinss)
print(km.out$betweenss)
print(km.out)
}
names(mr) <- c("class", "capshape", "capsurface", "capcolor","bruises", "odor","gillattachment",
"gillspacing","gillsize","gillcolor","stalkshape","stalkroot","stalksurfaceabovering",
"stalksurfacebelowring","stalkcolorabovering","stalkcolorbelowring","veiltype",
"veilcolor","ringnumber","ringtype","sporeprintcolor","population","habitat")
#data exploration
numeric <- mr
for (i in names(numeric)){
numeric[,i] <- as.numeric(numeric[,i])
}
str(numeric)
set.seed(12345)
initial <- numeric
row <- nrow(initial)
col <- col(initial)
trainindex <- sample(row, 0.7*row, replace=FALSE)
train_initial <- initial[trainindex,]
test_initial <- initial[-trainindex,]
dim(train_initial)
wss_and_bss(train_initial)
kmeans_iyer(train_initial,4)
version()
version
str(train_initial)
wss <- (nrow(train_initia)-1)*sum(apply(train_initia,2,var))
for (i in 1:12) wss[i] <- sum(kmeans(train_initia,
centers=i)$withinss)
print(plot(1:12, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares"))
wss <- (nrow(train_initial)-1)*sum(apply(train_initia,2,var))
for (i in 1:12) wss[i] <- sum(kmeans(train_initia,
centers=i)$withinss)
print(plot(1:12, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares"))
wss <- (nrow(train_initial)-1)*sum(apply(train_initial,2,var))
for (i in 1:12) wss[i] <- sum(kmeans(train_initial,
centers=i)$withinss)
print(plot(1:12, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares"))
wss <- (nrow(train_initial)-1)*sum(apply(train_initial,2,var))
for (i in 1:12) wss[i] <- sum(kmeans(train_initial,
centers=i)$withinss)
print(plot(1:12, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares"))
set.seed(12345)
row <- nrow(initial)
col <- col(initial)
#train/test 70-30
trainindex <- sample(row, 0.7*row, replace=FALSE)
train_initial <- initial[trainindex,]
test_initial <- initial[-trainindex,]
dim(train_initial)
str(train_initial)
wss <- (nrow(train_initial)-1)*sum(apply(train_initial,2,var))
for (i in 1:12) wss[i] <- sum(kmeans(train_initial,
centers=i)$withinss)
print(plot(1:12, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares"))
##plot wss and bss
wss_and_bss(train_initial)
## optimum number here is 4 clusters
kmeans_iyer(train_initial,4)
kmeans_iyer(train_initial,4)
kmeans_iyer(train_initial,4)
setwd("~/Desktop/GW_spring2016/data analysis/project 1")
setwd("~/Desktop/GW_spring2016/data analysis/project 1")
x <- read.csv('X.csv')
View(x)
setwd("~/Desktop/GW_spring2016/bigdata")
library(MASS)
library(dplyr)
library(rvgtest)
library(Hmisc)
library(reshape2)
library(caret)
library(FactoMineR)
library(ggplot2)
library(GGally)
install.packages("GGally")
library(MASS)
library(dplyr)
library(rvgtest)
library(Hmisc)
library(reshape2)
library(caret)
library(FactoMineR)
library(ggplot2)
library(GGally)
setwdsetwd("~/Desktop/GW_spring2016/bigdata")
mr <- read.table('agaricus-lepiota.data',sep=",",header=FALSE)
str(mr)
scale <- function(df){
pre_range <- preProcess(df,method="range")
processed <- predict(pre_range,df)
return(data.frame(processed))
}
wss_and_bss <- function(df){
#within sum of squares
wss <- (nrow(df)-1)*sum(apply(df,2,var))
for (i in 1:12) wss[i] <- sum(kmeans(df,
centers=i)$withinss)
print(plot(1:12, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares"))
#between sum of squares
bss <- (nrow(df)-1)*sum(apply(df,2,var))
for (i in 1:12) bss[i] <- sum(kmeans(df,
centers=i)$betweenss)
print(plot(1:12, bss, type="b", xlab="Number of Clusters",
ylab="Between groups sum of squares"))
}
#define function for final output
kmeans_iyer <- function(df,k,n=20){
fit <-kmeans(df,k)
#get cluster means:this illustrates amount of each characteristic in each cluster
aggregate(df,by=list(fit$cluster), FUN=mean)
#append cluster assignment
cluster_assignment <- data.frame(df, fit$cluster)
#table of how many variables fit in each cluster
#print(table(fit$cluster))
km.out<- kmeans(cluster_assignment,k,nstart=n)
print(km.out$tot.withinss)
print(km.out$betweenss)
print(km.out)
}
names(mr) <- c("class", "capshape", "capsurface", "capcolor","bruises", "odor","gillattachment",
"gillspacing","gillsize","gillcolor","stalkshape","stalkroot","stalksurfaceabovering",
"stalksurfacebelowring","stalkcolorabovering","stalkcolorbelowring","veiltype",
"veilcolor","ringnumber","ringtype","sporeprintcolor","population","habitat")
head(mr)
numeric <- mr
numeric[numeric == '?'] <- NA ##Protect the null values
numeric <- as.data.frame(sapply(numeric, as.numeric))
str(numeric)
summary(numeric)
cluster <- mr.t
mr_nzv <- subset( mr_nzv, select = -c(class) )
nzv <- nearZeroVar(numeric, saveMetrics = TRUE)
mr_nzv <- numeric[,c(rownames(nzv[nzv$nzv == FALSE,]))]
mr_nzv <- subset( mr_nzv, select = -c(class) )
pr <- PCA( mr_nzv, ncp = dim(mr_nzv)[2])
index <- max(which(pr$eig$eigenvalue > 1))
mr.t <- pr$ind$coord[,1:index]
mr.t <- as.data.frame(mr.t)
cluster <- mr.t
cluster$vtype <- NULL
row <- nrow(cluster)
col <- col(cluster)
set.seed(12345)
trainindex <- sample(row, 0.5*row, replace=FALSE)
train_50 <- cluster[trainindex,]
test_50 <- cluster[-trainindex,]
dim(train_50)
trainindex1 <- sample(row, 0.6*row, replace=FALSE)
train_60 <- cluster[trainindex,]
test_60 <- cluster[-trainindex,]
dim(train_60)
trainindex1 <- sample(row, 0.6*row, replace=FALSE)
train_60 <- cluster[trainindex1,]
test_60 <- cluster[-trainindex1,]
dim(train_60)
trainindex2 <- sample(row, 0.7*row, replace=FALSE)
train_70 <- cluster[trainindex2,]
test_70 <- cluster[-trainindex2,]
dim(train_70)
train1 <- scale(train_50)
test1 <- scale(test_50)
##turn both into matrix
train1 <- as.matrix(train1)
test1 <- as.matrix(test1)
##range
train2 <- scale(train_60)
test2 <- scale(test_60)
##turn both into matrix
train2 <- as.matrix(train2)
test2 <- as.matrix(test2)
train3 <- scale(train_70)
test3 <- scale(test_70)
##turn both into matrix
train3 <- as.matrix(train3)
test3 <- as.matrix(test3)
hc.complete2=hclust(dist(train2), method="complete")
hc.average2=hclust(dist(train2), method="average")
hc.single2=hclust(dist(train2), method="single")
plot(hc.complete2)
table(numeric$class)
ind <- sample(2, nrow(numeric), replace=TRUE, prob=c(.70,.30))
knn.train <- numeric[ind==1,2:23]
knn.test <- numeric[ind==2,2:23]
knn.trainLabels <- numeric[ind==1,1]
knn.testLabels <- numeric[ind==2,1]
knn_pred <- knn(train = knn.train, test = knn.test, cl = knn.trainLabels, k=10)
library(class)
library(gmodels)
knn_pred <- knn(train = knn.train, test = knn.test, cl = knn.trainLabels, k=10)
knn_pred
ind <- sample(2, nrow(numeric), replace=TRUE, prob=c(.67,.33))
knn.train <- numeric[ind==1,2:23]
knn.test <- numeric[ind==2,2:23]
#train and test labels for target var
knn.trainLabels <- numeric[ind==1,1]
knn.testLabels <- numeric[ind==2,1]
#k nearest on training using 10 as k
knn_pred <- knn(train = knn.train, test = knn.test, cl = knn.trainLabels, k=10)
str(numeric)
sum(is.na(knn.test))
sum(is.na(numeric))
library(randomForest)
knn.train <- na.roughfix(knn.train)
knn.test <- na.roughfix(knn.test)
knn.trainLabels <- numeric[ind==1,1]
knn.testLabels <- numeric[ind==2,1]
knn_pred <- knn(train = knn.train, test = knn.test, cl = knn.trainLabels, k=10)
knn_pred
CrossTable(x = knn.testLabels, y = knn_pred, prop.chisq=FALSE)
#true negative/true positive = model score
ind <- sample(2, nrow(numeric), replace=TRUE, prob=c(.70,.30))
knn.train <- numeric[ind==1,2:23]
knn.test <- numeric[ind==2,2:23]
#first rough fix NAs
library(randomForest)
knn.train <- na.roughfix(knn.train)
knn.test <- na.roughfix(knn.test)
#train and test labels for target var
knn.trainLabels <- numeric[ind==1,1]
knn.testLabels <- numeric[ind==2,1]
#k nearest on training using 10 as k
knn_pred <- knn(train = knn.train, test = knn.test, cl = knn.trainLabels, k=10)
knn_pred
knn_all <- function(df,p1,p2,k=10){
#training and test for knn
ind <- sample(2, nrow(df, replace=TRUE, prob=c(p1,p2))
knn.train <- df[ind==1,2:23]
knn.test <- df[ind==2,2:23]
#first rough fix NAs
library(randomForest)
knn.train <- na.roughfix(knn.train)
knn.test <- na.roughfix(knn.test)
#train and test labels for target var
knn.trainLabels <- numeric[ind==1,1]
knn.testLabels <- numeric[ind==2,1]
#k nearest on training using 10 as k
knn_pred <- knn(train = knn.train, test = knn.test, cl = knn.trainLabels, k)
#cross table
return(CrossTable(x = knn.testLabels, y = knn_pred, prop.chisq=FALSE))
}
knn_all <- function(df,p1,p2,k=10){
#training and test for knn
ind <- sample(2, nrow(df, replace=TRUE, prob=c(p1,p2))
knn.train <- df[ind==1,2:23]
knn.test <- df[ind==2,2:23]
#first rough fix NAs
library(randomForest)
knn.train <- na.roughfix(knn.train)
knn.test <- na.roughfix(knn.test)
#train and test labels for target var
knn.trainLabels <- numeric[ind==1,1]
knn.testLabels <- numeric[ind==2,1]
#k nearest on training using 10 as k
knn_pred <- knn(train = knn.train, test = knn.test, cl = knn.trainLabels, k)
#cross table
return(CrossTable(x = knn.testLabels, y = knn_pred, prop.chisq=FALSE))
}
knn_all <- function(df,p1,p2,k=10){
#training and test for knn
ind <- sample(2, nrow(df, replace=TRUE, prob=c(p1,p2)))
knn.train <- df[ind==1,2:23]
knn.test <- df[ind==2,2:23]
#first rough fix NAs
library(randomForest)
knn.train <- na.roughfix(knn.train)
knn.test <- na.roughfix(knn.test)
#train and test labels for target var
knn.trainLabels <- numeric[ind==1,1]
knn.testLabels <- numeric[ind==2,1]
#k nearest on training using 10 as k
knn_pred <- knn(train = knn.train, test = knn.test, cl = knn.trainLabels, k)
#cross table
return(CrossTable(x = knn.testLabels, y = knn_pred, prop.chisq=FALSE))
}
library(class)
library(gmodels)
str(numeric)
knn_all(numeric,p1 = .7,p2 = .3)
knn_all(numeric,p1 = .7,p2 = .3,k)
knn_all(numeric,.7,.3,10)
knn_all <- function(df,p1,p2,k=10){
p1 = as.numeric(p1)
p2 = as.numeric(p2)
#training and test for knn
ind <- sample(2, nrow(df, replace=TRUE, prob=c(p1,p2)))
knn.train <- df[ind==1,2:23]
knn.test <- df[ind==2,2:23]
#first rough fix NAs
library(randomForest)
knn.train <- na.roughfix(knn.train)
knn.test <- na.roughfix(knn.test)
#train and test labels for target var
knn.trainLabels <- numeric[ind==1,1]
knn.testLabels <- numeric[ind==2,1]
#k nearest on training using 10 as k
knn_pred <- knn(train = knn.train, test = knn.test, cl = knn.trainLabels, k)
#cross table
return(CrossTable(x = knn.testLabels, y = knn_pred, prop.chisq=FALSE))
}
knn_all <- function(df,p1,p2,k=10){
p1 = as.numeric(p1)
p2 = as.numeric(p2)
#training and test for knn
ind <- sample(2, nrow(df, replace=TRUE, prob=c(p1,p2)))
knn.train <- df[ind==1,2:23]
knn.test <- df[ind==2,2:23]
#first rough fix NAs
library(randomForest)
knn.train <- na.roughfix(knn.train)
knn.test <- na.roughfix(knn.test)
#train and test labels for target var
knn.trainLabels <- numeric[ind==1,1]
knn.testLabels <- numeric[ind==2,1]
#k nearest on training using 10 as k
knn_pred <- knn(train = knn.train, test = knn.test, cl = knn.trainLabels, k)
#cross table
return(CrossTable(x = knn.testLabels, y = knn_pred, prop.chisq=FALSE))
}
knn_all(numeric,.7,.3,10)
knn_all <- function(df,k=10){
library(randomForest)
knn.train <- na.roughfix(knn.train)
knn.test <- na.roughfix(knn.test)
#train and test labels for target var
knn.trainLabels <- numeric[ind==1,1]
knn.testLabels <- numeric[ind==2,1]
#k nearest on training using 10 as k
knn_pred <- knn(train = knn.train, test = knn.test, cl = knn.trainLabels, k)
#cross table
return(CrossTable(x = knn.testLabels, y = knn_pred, prop.chisq=FALSE))
}
knn_all <- function(df,k=10){
library(randomForest)
knn.train <- na.roughfix(knn.train)
knn.test <- na.roughfix(knn.test)
#train and test labels for target var
knn.trainLabels <- numeric[ind==1,1]
knn.testLabels <- numeric[ind==2,1]
#k nearest on training using 10 as k
knn_pred <- knn(train = knn.train, test = knn.test, cl = knn.trainLabels, k)
#cross table
return(CrossTable(x = knn.testLabels, y = knn_pred, prop.chisq=FALSE))
}
ind <- sample(2, nrow(numeric), replace=TRUE, prob=c(.70,.30))
knn.train <- numeric[ind==1,2:23]
knn.test <- numeric[ind==2,2:23]
#first rough fix NAs
library(randomForest)
knn.train <- na.roughfix(knn.train)
knn.test <- na.roughfix(knn.test)
#train and test labels for target var
knn.trainLabels <- numeric[ind==1,1]
knn.testLabels <- numeric[ind==2,1]
ind1 <- sample(2, nrow(numeric), replace=TRUE, prob=c(.50,.50))
knn.train1 <- numeric[ind1==1,2:23]
knn.test1 <- numeric[ind1==2,2:23]
#first rough fix NAs
library(randomForest)
knn.train1 <- na.roughfix(knn.train1)
knn.test1 <- na.roughfix(knn.test1)
#train and test labels for target var
knn.trainLabels1 <- numeric[ind1==1,1]
knn.testLabels1 <- numeric[ind1==2,1]
#k nearest on training using 10 as k
knn_pred1 <- knn(train = knn.train1, test = knn.test1, cl = knn.trainLabels1, k=10)
#cross table
CrossTable(x = knn.testLabel1s, y = knn_pred1, prop.chisq=FALSE)
ind1 <- sample(2, nrow(numeric), replace=TRUE, prob=c(.50,.50))
knn.train1 <- numeric[ind1==1,2:23]
knn.test1 <- numeric[ind1==2,2:23]
#first rough fix NAs
library(randomForest)
knn.train1 <- na.roughfix(knn.train1)
knn.test1 <- na.roughfix(knn.test1)
#train and test labels for target var
knn.trainLabels1 <- numeric[ind1==1,1]
knn.testLabels1 <- numeric[ind1==2,1]
#k nearest on training using 10 as k
knn_pred1 <- knn(train = knn.train1, test = knn.test1, cl = knn.trainLabels1, k=10)
#cross table
CrossTable(x = knn.testLabels1, y = knn_pred1, prop.chisq=FALSE)
set.seed(12345)
#training and test for knn (50-50 train-test)
ind1 <- sample(2, nrow(numeric), replace=TRUE, prob=c(.50,.50))
knn.train1 <- numeric[ind1==1,2:23]
knn.test1 <- numeric[ind1==2,2:23]
#first rough fix NAs
library(randomForest)
knn.train1 <- na.roughfix(knn.train1)
knn.test1 <- na.roughfix(knn.test1)
#train and test labels for target var
knn.trainLabels1 <- numeric[ind1==1,1]
knn.testLabels1 <- numeric[ind1==2,1]
#k nearest on training using 10 as k
knn_pred1 <- knn(train = knn.train1, test = knn.test1, cl = knn.trainLabels1, k=10)
#cross table
CrossTable(x = knn.testLabels1, y = knn_pred1, prop.chisq=FALSE)
ind2 <- sample(2, nrow(numeric), replace=TRUE, prob=c(.60,.40))
knn.train2 <- numeric[ind2==1,2:23]
knn.test2 <- numeric[ind2==2,2:23]
#first rough fix NAs
library(randomForest)
knn.train2 <- na.roughfix(knn.train2)
knn.test2 <- na.roughfix(knn.test2)
#train and test labels for target var
knn.trainLabels2 <- numeric[ind2==1,1]
knn.testLabels2 <- numeric[ind2==2,1]
#k nearest on training using 10 as k
knn_pred2 <- knn(train = knn.train2, test = knn.test2, cl = knn.trainLabels2, k=10)
#cross table
CrossTable(x = knn.testLabels2, y = knn_pred2, prop.chisq=FALSE)
ind3 <- sample(2, nrow(numeric), replace=TRUE, prob=c(.70,.30))
knn.train3 <- numeric[ind3==1,2:23]
knn.test3 <- numeric[ind3==2,2:23]
#first rough fix NAs
library(randomForest)
knn.train3 <- na.roughfix(knn.train3)
knn.test3 <- na.roughfix(knn.test3)
#train and test labels for target var
knn.trainLabels3 <- numeric[ind3==1,1]
knn.testLabels3 <- numeric[ind3==2,1]
#k nearest on training using 10 as k
knn_pred3 <- knn(train = knn.train3, test = knn.test3, cl = knn.trainLabels3, k=10)
#cross table
CrossTable(x = knn.testLabels3, y = knn_pred3, prop.chisq=FALSE)
